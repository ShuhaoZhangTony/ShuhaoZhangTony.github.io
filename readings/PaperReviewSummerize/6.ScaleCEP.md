# Salable CEP system

##Background
After the first study of “partitioning techniques for CEP”, I understand the basic techniques to partition the CEP, or to scale out the CEP, then, how to fully use those techniques to achieve better performance?
##Rely on special hardware
###GPU & FPGA
1. In "Parallelization of Complex Event Processing on GPU", the author implement three parallel approaches,
	- A.Thread per event.
	- B.Thread per subscriber.
	- C.Thread per event-subscriber pair.
> The target system is public-subscribe system (not exactly CEP system), but the idea is somehow inspiring.

2. In "High performance FPGA and GPU complex pattern matching over spatio-temporal streams", the author proposed two main parallelism level approaches:
	- Inter-pattern parallelism : all pattern queries are evaluated in parallel. 
	- Intra-pattern parallelism : individual predicates within a pattern are evaluated in parallel on different SPs. 
	>the idea in intra-pattern parallelism similar are very similar to "Scalable Complex Event Processing on Top of MapReduce", that the <Map> is doing filtering(predicates).

The above paper proposed a series optimization strategies specific to FPGA/GPU as well, but out of my interest.

##Rely on other framework

1. In ICCSPS'14, "High Efficient Complex Event Processing Based on Storm" proposed their efforts on implementing complex event processing system on storm, in dueling with the issue "supporting complex event detection in multiple sources environments". They hope to go further than distributing queries and achieves better scalability by parallelizing event detection.[But suspected to be based on pattern partitioning.] idea like bellow:
![cepmapreduce](figure\ceponstorm.png)
According to the paper, the author treat spout and bolt as different operators. However, the implementation and experiment detail is unclear. 

2. In APWeb'12, "Scalable Complex Event Processing on Top of MapReduce" proposed their efforts of building complex event processing framework on top of MapReduce. They claimed that although many efforts are put to modify the original mapReduce idea to support different applications, none of them support CEP. Their basic idea looks like the figure bellow:

![cepmapreduce](figure\cepmapreduce.png)

- **Map** task is to *filter* events to reduce the cost of the sort and group phase.
- **Partition** uses a hash function on the attribute ID to ensure that the events with the same ID will be processed in one reduce task.
- **Sort** will sort the data according to the primary key of events.
- **Reduce** is to to match and identify events gradually for each group.

##New semantic architecture
>Lots of relevant work in network community

Usually, CEP is developed on client-server style. (We can distribute server of-course, but it's still CS-model).
In Intelligent Distributed Computing VII' 14, "Semantically Partitioned Peer to Peer Complex Event Processing", 
> **Some nice sentences from author:** " Events maybe correlated together and may thusly cross both technological and domain boundaries. The process of correlating is referred to as **pattern matching**.
> 
> Motivation for CEP nicely correlates with the current advent of Big Data, which is a data centric approach to extracting meaningful data. The problem here is not to store such data, but to retrieve it and to extract meaningful information.
> 
> In CEP, data that's not processed is simply discarded.

their idea is simply show as bellow:

![cepmapreduce](figure\p2pcep.png)
they also proposed two heuristic algorithm , the algorithm dynamically add/remove CEP engines among peers to conduct pattern matching.

##Replicated engine
>No prior work of CEP on this topic

In EuroSys'11, "Database Engines on Multicores, Why Parallelize When You Can Distribute?", the author shows that:"Deploy several replicated engines within a single multicore machine can achieve better scalability and stability than a single database engine operating on all cores."
The author rather than redesigning the engine, they partition the *multicore* machine and allocate an unmodified database engine(PosgreSQL and MYSQL) to each partition. specially, their design belongs to "Single master replication with support for speciallized satellites and partial replication.

##Elastic
>Tomas hanizen. Since it’s hard to figure out the capacity needed in streaming environment. (How many msg per second).

- How to manual provision it.
- How to automatically tuning it.
- How to control the partition the original data processing flow to allow elastic. But the number of partition is defined in CCL (up to the user). You have to figure it out.
- How to due with the wait stage before start the optimized plan.


##Speculative
###Background
I have no background knowledge for this topic, I suspect in Database, there must be many similar idea but not exactly usable in CEP context.

###Relevant work
Speculative **out-of-order** event processing with software transaction memory
There are many existing works for “Guessing” on stream processing. The major motivation is “out-of-order”.
Even though my motivation is a bit different, which is to improve the utilization of CEP engine, thus the challenge include “the measurement of utilization” +”Speculative”, I don’t know whether it’s worth to research it again or not.

###Motivation
Say we “know” a event stream “normally” follows following pattern:[A..A10,B..B10,C];
Say we want to compute following query: SEQ(A+,B+,C) WHERE [id], GROUP BY[id], SUM[∑"B" +C].
And we know C is an integer fall in [0,1,2].
Say now we have [A..A10,B..B10] already. and we see the engine is under untilize, we thus want to fully utilize the engine, thus we can “guess” what’s the value of the followed C. and followed by a pre-computation, when C really comes, we just need to check whether our guess is correct or not, if correct, we done, otherwise we need to follow normal procedure and compute.
###Two challenges
how to determine whether the engine is really under-utilize thus worth to do the pre-computation and also challenge of how to “guess”.

##Event Cloud Processing
###Background
With AWS Lambda, you pay only for what you use. You are charged based on the number of requests for your functions and the time your code executes. The Lambda free tier includes 1M free requests per month and 400,000 GB-seconds of compute time per month. 
####Idea
Reduce the number of requests equal to reduce costs.
one way to reduce the number of requests is to maximumly sharing the intermediate results. But it may violate QoS. The challenge is to find the Optimal point that satisfied the constraints.
