\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{exscale}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\ra}{\rightarrow}

\setlength{\parindent}{0pt}
\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0.3in}
\setlength\textheight{8.5in}
\setlength\textwidth{6.3in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength{\tabcolsep}{3pt}
\setlength\parskip{1ex plus 0.5ex minus 0.2ex}

\title{MAS 433 Tutorial 1}
\author{Wang Xueou (087199E16)}
\begin{document}
\maketitle

\textbf{Question 1 } Solution:\\
\textbf{1.1.} In the modified one-time pad $C_i$ is either $0$, $1$, or $2$.\\
If $C_i=0$, we know both the plaintext and the key are $0$, i.e., $P_i=0, K_i=0$.\\
If $C_i=2$, then both the plaintext and the key are $0$, i.e., $P_i=1,K_i=1$. \\
If $C_i=1$, we have either $P_i=0, K_i=1$, or $P_i=1, K_i=0$. As there are only 2 possibilities, we just try each case and check in which case the plaintext makes sense.\\
\textbf{1.2.} A cryptosystem has perfect secrecy if knowing ciphertext reveals no information about the plaintext. i.e., $ \textbf{Pr} \left[ \textbf{P}=p | \textbf{C}=c\right]=\textbf{Pr} \left[ \textbf{P}=p\right]$. Now, suppose $\textbf{P}=\textbf{K}=\left\{ 0,1\right\}^n$. After encrypting using the modified one-time pad, the cipher \textbf{C} has $m$ 1's and the rest 0's $\left( 0 < m < n \textnormal{ because } m=0 \textnormal{ or } n \textnormal{ is trivial} \right)$.
Then we have:\\
$\textbf{Pr} \left[ \textbf{P}=p \right] = \frac{1}{2^n}$\\
while $\textbf{Pr} \left[ \textbf{P}=p | \textbf{C}=c \right] = \frac{1}{2^m} > \textbf{Pr} \left[ \textbf{P}=p \right] $\\
Thus, the modified one-time pad is not perfectly secure.\\



\textbf{Question 2 } Solution:\\
\textbf{2.1.}
$\textnormal{Entropy} = - 2^{995} \times \left( \frac{1}{2^{995}}  \times \log_2 \frac{1}{2^{995}}\right) = 995$\\
The risk of using the key is that the first five bits will be known to the attacker. If there are sensitive information in the five bits, it would be very dangerous.\\
\textbf{2.2.}
$\textnormal{Entropy} = -1000 \times \left( 0.54 \times \log_2 0.54 + 0.46 \times \log_2 0.46 \right) = 995.378439$\\
The risk of using the key is that the key will have around $54\%$ of zeros. Suppose the computer is subjected to unlimited computing ability, then all the keys with 540 zeros can be tried to break the cipher.\\ 


\textbf{Question 3 } Solution:\\
\textbf{3.1. }
$\textnormal{Entropy} = 128 \times \left( -0.5 \times \log_2 0.5 -0.5 \times \log_2 0.5 \right) = 128$\\
On average, the guesses needed to guess the value correctly is:\\
$$
\begin{array}{cl}
 & \sum_{i=0}^{2^{128}-1}{ \frac{2^{128} - i}{2^{128}} \times \frac{1}{2^{128} - i} \times \left( i+1 \right)} \\
 =& \sum_{i=0}^{2^{128}-1}{ \frac{i+1}{2^{128}}} \\
 =& \frac{1}{2^{128}} \times \frac{ \left( 1 + 2^{128} \right) \times 2^{128}}{2} \\
 =& 2^{127} + 0.5 \\
\end{array}
$$

\textbf{3.2. }\\
\textbf{3.2.1. } 
$\textnormal{Entropy} = -\frac{1}{2} \times \log_2 \frac{1}{2} - 2^{127} \times \left( \frac{\frac{1}{2}}{2^{127}} \log_2 \frac{\frac{1}{2}}{2^{127}} \right) = 127\frac{1}{2}$\\
\textbf{3.2.2. } Since $\lq \lq010101\cdots0101\rq \rq$ appears half of the time, we have half of all the keys being this key. Thus we only need to guess the which half of the keys is this key. We have\\
$$
\left( \frac{1}{2} \right)^{\frac{n}{2}} \frac{n}{2} + \sum_{i=1}^{\frac{n}{2}-1}{\left( \begin{array}{c}\frac{n}{2}\\i \end{array}\right) \left( \frac{1}{2}\right)^i \left( \frac{1}{2}\right)^{\frac{n}{2}-i} \left( \frac{n}{2}+i\right)} 
$$


\textbf{3.2.3. } The average number of guesses is
$$
\begin{array}{cl}
 & \frac{1}{2} \times 1 + \sum_{i=0}^{2^{127}-1}{ \frac{2^{127}-i}{2^{127}} \times \frac{1}{2^{127}-i} \times \left( i+1 \right})\\
=& 0.5 + \frac{1}{2^{127}} \sum_{i=0}^{2^{127}-1} {\frac{i+2}{2^{127}}} \\
=& 0.5 + \frac{1}{2^{127}} \times \frac{ \left(2+2^{127}+1 \right) \times 2^{127}}{2} \\
=& 0.5 + \left( 1.5 + 2^{126}\right)\\
=& 2^{126} + 2 \\
\end{array}
$$


\textbf{Question 4 } Solution:\\
\textbf{4.1.} $ \textnormal{\textbf{Pr}} \left[ C = c_i \right] = \Sigma_{j,s}\textnormal{\textbf{Pr}} \left[ P = p_j\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_s\right], \textnormal{where} E_{k_s}\left( p_s \right) = c_i $. Thus, \\
\begin{tabular}{l}
$\textnormal{\textbf{Pr}} \left[ C = c_1\right] = \textnormal{\textbf{Pr}} \left[ P = p_1\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_1 \right] = \frac{1}{4} \times \frac{1}{2} = \frac{1}{8}$\\


$\textnormal{\textbf{Pr}} \left[ C = c_2\right] = \textnormal{\textbf{Pr}} \left[ P = p_1\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_2 \right] + \textnormal{\textbf{Pr}} \left[ P = p_2\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_1 \right] = \frac{1}{4} \times \frac{1}{4} + \frac{3}{4} \times \frac{1}{2} = \frac{7}{16}$\\


$\textnormal{\textbf{Pr}} \left[ C = c_3\right] = \textnormal{\textbf{Pr}} \left[ P = p_1\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_3 \right] + \textnormal{\textbf{Pr}} \left[ P = p_2\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_2 \right] =
\frac{1}{4} \times \frac{1}{4} + \frac{3}{4} \times \frac{1}{4} = \frac{1}{4}$\\


$\textnormal{\textbf{Pr}} \left[ C = c_4\right] = \textnormal{\textbf{Pr}} \left[ P = p_2\right] \cdot \textnormal{\textbf{Pr}} \left[ K = k_3 \right] = \frac{3}{4} \times \frac{1}{4} = \frac{3}{16} $
\end{tabular}

\textbf{4.2.}\\
\begin{tabular}{l}
$\textnormal{Entropy of \textbf{P}} = -\frac{1}{4} \times \log_2 \frac{1}{4} - \frac{3}{4} \times \log_2 \frac{3}{4} = 0.811278124 $\\


$\textnormal{Entropy of \textbf{K}} = -\frac{1}{2} \times \log_2 \frac{1}{2} - \frac{1}{4} \times \log_2 \frac{1}{4} - \frac{1}{4} \times \frac{1}{4} = 1.5$\\


$\textnormal{Entropy of \textbf{C}} = -\frac{1}{8} \times \log_2 \frac{1}{8} - \frac{7}{16} \times \log_2 \frac{7}{16} - 
\frac{1}{4} \times \log_2 \frac{1}{4} - \frac{3}{16} \times \log_2 \frac{3}{16} = 1.84960175$
\end{tabular}

\textbf{4.3.}\\
$\textnormal{\textbf{Pr}} \left(p_1|c_1 \right) = \frac{\textnormal{\textbf{Pr}}\left( P=p_1, C=c_1\right)}{P \left( C=c_1\right)} = \frac{\textnormal{\textbf{Pr}} \left( P=p_1\right)\textnormal{\textbf{Pr}} \left( K=k_1\right)}{P\left( C=c_1\right)} = \frac{\frac{1}{4} \times \frac{1}{2}}{\frac{1}{8}} = 1 $\\
$\textnormal{\textbf{Pr}} \left(p_1|c_2 \right) = \frac{\textnormal{\textbf{Pr}}\left( P=p_1, C=c_2\right)}{P \left( C=c_1\right)} = \frac{\textnormal{\textbf{Pr}} \left( P=p_1\right)\textnormal{\textbf{Pr}} \left( K=k_2\right)}{P\left( C=c_2\right)} = \frac{\frac{1}{4} \times \frac{1}{4}}{\frac{7}{16}} = \frac{1}{7}$\\
$\textnormal{\textbf{Pr}} \left(p_1|c_3 \right) = \frac{\textnormal{\textbf{Pr}}\left( P=p_1, C=c_3\right)}{P \left( C=c_3\right)} = \frac{\textnormal{\textbf{Pr}} \left( P=p_1\right)\textnormal{\textbf{Pr}} \left( K=k_3\right)}{P\left( C=c_3\right)} = \frac{\frac{1}{4} \times \frac{1}{4}}{\frac{1}{4}} = \frac{1}{4}$\\
$\textnormal{\textbf{Pr}} \left(p_1|c_4 \right) = \frac{\textnormal{\textbf{Pr}}\left( P=p_1, C=c_4\right)}{P \left( C=c_4\right)} = 0$\\
$\textnormal{\textbf{Pr}} \left(p_2|c_1 \right) = 1-\textnormal{\textbf{Pr}} \left(p_1|c_1 \right) = 0 $\\
$\textnormal{\textbf{Pr}} \left(p_2|c_2 \right) = 1-\textnormal{\textbf{Pr}} \left(p_1|c_2 \right) = \frac{6}{7} $\\
$\textnormal{\textbf{Pr}} \left(p_2|c_3 \right) = 1-\textnormal{\textbf{Pr}} \left(p_1|c_3 \right) = \frac{3}{4} $\\
$\textnormal{\textbf{Pr}} \left(p_2|c_4 \right) = 1-\textnormal{\textbf{Pr}} \left(p_1|c_4 \right) = 1 $\\

\textbf{4.4.}\\
$$
\begin{array}{lcl}
\textnormal{Given } c_1: \textnormal{Entropy of \textbf{P}} &=& -\textnormal{\textbf{Pr}} \left(p_1|c_1 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_1|c_1 \right) -\textnormal{\textbf{Pr}} \left(p_2|c_1 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_2|c_1 \right) \\
&=& 0 + 0 \\
&=& 0 \\
\textnormal{Given } c_2: \textnormal{Entropy of \textbf{P}} &=& -\textnormal{\textbf{Pr}} \left(p_1|c_2 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_1|c_2 \right) -\textnormal{\textbf{Pr}} \left(p_2|c_2 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_2|c_2 \right) \\
&=& -\frac{1}{7} \times \log_2 \frac{1}{7} - \frac{6}{7} \times \log_2 \frac{6}{7} \\
&=& 0.591672779  \\
\textnormal{Given } c_3: \textnormal{Entropy of \textbf{P}} &=& -\textnormal{\textbf{Pr}} \left(p_1|c_3 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_1|c_3 \right) -\textnormal{\textbf{Pr}} \left(p_2|c_3 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_2|c_3 \right) \\
&=& -\frac{1}{4} \times \log_2 \frac{1}{4} - \frac{3}{4} \times \log_2 \frac{3}{4} \\
&=& 0.811278124  \\
\textnormal{Given } c_4: \textnormal{Entropy of \textbf{P}} &=& -\textnormal{\textbf{Pr}} \left(p_1|c_4 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_1|c_4 \right) -\textnormal{\textbf{Pr}} \left(p_2|c_4 \right) \log_2 \textnormal{\textbf{Pr}} \left(p_2|c_4 \right) \\
&=& 0 +0 \\
&=& 0  \\
\end{array}
$$
These results are different from the entropy of \textbf{P} because the keys are not randomly generated and thus they are not perfectly secure, i.e. $\textnormal{\textbf{Pr}} \left[ P = p_i|C = c_j\right] \neq \textnormal{\textbf{Pr}} \left[ P = p_i\right]$\\


\textbf{Question 5 } Solution:\\
For plaintext spaces consisting of $26^m$ $m-$ grams, we have $\left| \textbf{K} \right| = {26^m} \!$, i.e. $n = 26^m$ \\
$
\begin{array}{lcl}
\log _2 n \! &\approx& \log_2 \left( \sqrt{2\pi n}\right) \left( \frac{n}{e}\right)^n)\\
 &=&\log_2 \sqrt{2 \pi} + 0.5 \log_2 n +n \log_2 \left( \frac{n}{e}\right)\\
 &=&\left( 0.5 + n\right)\log_2 n - n \log_2 e + \log_2 \sqrt{2 \pi}\\
 &=&\left( 0.5+n\right) \log_2 - 1.44n +1.33 \\
\end{array}
$\\
The unicity distance is defined by
$$
n_0 \approx \frac{ \log_2 \left| K \right| }{R_L \log_2 \left| P\right|}
$$
Now $n = 26^m$,
$$
\begin{array}{lcl}
n_0 &\approx& \frac{\left( 0.5 + 26^m\right) \log_2 26^m 1.44 \times 26^m + 1.33}{R_L \log_2 \left| P\right|}\\
&=& \frac{m\left( 0.5 + 26^m \right) \log_2 26 - 1.44 \times 26^m + 1.33}{0.75 \times \log_2 26}\\
&=& \frac{m\left( 0.5 + 26^m \right) \log_2 26 - 1.44 \times 26^m + 1.33}{0.75 \times 4.7}\\
\end{array}
$$
If $m=1, n_0 = \frac{\left( 0.5 + 26 \right) \log_2 26 -1.44 \times 26 + 1.33}{0.75 \times 4.7} = 25.0926674$\\
If $m=2, n_0 = \frac{2 \times \left( 0.5 + 26^2 \right) \log_2 26 -1.44 \times 26^2 + 1.33}{0.75 \times 4.7} = 1 528.39289$\\
If $m=3, n_0 = \frac{3 \times \left( 0.5 + 26^3 \right) \log_2 26 -1.44 \times 26^3 + 1.33}{0.75 \times 4.7} = 63 132.9719$\\
If $m=4, n_0 = \frac{4 \times \left( 0.5 + 26^4 \right) \log_2 26 -1.44 \times 26^4 + 1.33}{0.75 \times 4.7} = 2 250 756.84$\\
If $m=5, n_0 = \frac{5 \times \left( 0.5 + 26^5 \right) \log_2{26^5} -1.44 \times 26^5 + 1.33}{0.75 \times 4.7} = 74 362 919.1$\\



\textbf{Question 6} Solution:\\
\textbf{P} = \textbf{C} = \textbf{K} = $\{0,1,2,\cdots,25 \}$, and key is chosen randomly. We need to prove $\textnormal{\textbf{Pr}} \left[ P = p | C = c\right] = \textnormal{\textbf{Pr}} \left[ P = p \right]$
\begin{proof}
$\textnormal{\textbf{Pr}} \left[ C = c | P = p\right] = \textnormal{\textbf{Pr}} \left[ K = c\ominus p \right] = \frac{1}{26}$\\
$
\begin{array}{lcl}
\textnormal{\textbf{Pr}} \left[ C = c \right] &=& \sum_{p\in \textnormal{\textbf{P}}} \left( \textnormal{\textbf{Pr}} \left[ P = p \right] \textnormal{\textbf{Pr}} \left[ C = c | P = p \right] \right)\\
&=& \sum_{p\in \textnormal{\textbf{P}}} \left( \textnormal{\textbf{Pr}} \left[ P = p \right] \cdot \frac{1}{26} \right)\\
&=& \frac{1}{26} 
\end{array}
$\\
$
\begin{array}{lcl}
\textnormal{\textbf{Pr}} \left[ P = p | C = c\right] &=& \frac{\textnormal{\textbf{Pr}} \left[ P = p \right] \cdot  \textnormal{\textbf{Pr}} \left[ C = c | P = p \right]}{\textnormal{\textbf{Pr}} \left[ C = c \right]}\\
&=& \frac{\textnormal{\textbf{Pr}} \left[ P = p \right] \cdot \frac{1}{26}}{\frac{1}{26}} \\
&=& \textnormal{\textbf{Pr}} \left[ P = p \right]
\end{array}
$\\


\textbf{Question 7} Solution:\\
\textbf{7.1. }\\
\textbf{Step1. }Suppose we guess the key length is $m$. We pick out the $1st, (1+m)th, (1+2m)th, \cdots$ to constitute a new cipher $C^{new}$. Then $C^{new}$ is just a shift cipher.\\
\textbf{Step2. }For $k = 0, 1, 2, \cdots, 25$, we shift $C^{new}$ using $k$ to get the decrypted message $D^{new}$.\\
\textbf{Step3. }Then we calculate the frequency of letter $A, B, C, \cdots, Z$ in $D^{new}$.\\
\textbf{Step4. }Let $X =(x_0, x_1, \cdots, x_{25}), Y=(y_0, y_1, \cdots, y_{25})$, where $x_i, y_i$ are the frequency of letter $i$ in $D^{new}$ and Englishi language respectively. We calculate the correlation between them as:\\
$$
Corr_{xy} = \frac{26 \sum{x_iy_i} - \sum{x_iy_i}}{\sqrt{26 \sum{x_i^2} - \left( \sum{x_i}\right)^2} \sqrt{26 \sum{y_i^2} - \left( \sum{y_i}\right)^2}}
$$
Then the smallest $Corr_{xy}$ indicates that our key length and the shift key are both correct.\\

\textbf{7.2. }Suppose we want to try the key length of $m$. Then we pick up the $i-th \left( 1 \leq i m \right)$ letter every $m$ letters in the original cipher and thus get $m$ new ciphers $C_1, C_2, \cdots, C_m $. For each new cipher $C_i$, we calculate the frequency of every cipher letter and get their entropy by 
$$
H\left( C_i\right) = - \sum_{x \in C_i} \textnormal{\textbf{Pr}} \left[ x \right] \cdot \log_2 \textnormal{\textbf{Pr}} \left[ x \right]
$$
If all the $m$ entropies are around 4.19, then our length is correct.  
\end{proof}



\end{document}