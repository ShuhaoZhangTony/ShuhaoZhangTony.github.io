\section{Design Details}
\label{sec:design}
%In this section, we discuss the designs of \algofull. 
%and explain their unique optimization techniques. 
%Then, we discuss \system's NUMA-awareness design, and how it achieves durability.
%As the scale and complexity of applications increase, it becomes more and more difficult to ensure the correct execution, while maintain high throughput low latency stream processing requirement, in the presence of concurrent accessing (read and write) over shared data sources. 
%Previous implementations are not free of scaling bottlenecks with a key reason being the requirement for tracking timestamp ordering of each event from input stream.
%To address this issue, we design and implement a new concurrent state management mechanism for stream processing on multicores -- \emph{bulk synchronous concurrency control} (\algo), which rely on a two-layered architecture that cleanly separate stream analytic and shared state management tasks. 
In this section, we discuss the detailed design of \system's concurrency control protocol, which targets at reducing the synchronization overhead for supporting concurrent stateful stream processing. 

% Enabling more concurrency has become particularly important with the proliferation of multicore and large-scale systems.
We introduce a dual-tier architecture (shown in Figure~\ref{fig:overview}(b)) that cleanly separates stream computation and state management procedures. 
The first layer is a stream processing layer (SP-Layer) managing pipelined and parallel stream computation, and the second layer is a transaction processing layer (TP-Layer) managing state management operations. 
%and also creates the potential of independent replacement of each components, e.g., we can upgrade the TP-Layer without changing SP-Layer in the future.
%External events that enter the system are first handled by \emph{SP-Layer}, where multiple operators run independently in a pipelined manner by sending output data to downstream operators. 
%Consequently, the read/write operations to the shared state generated during operator execution will be handled by \emph{TP-Layer}. 
%Both layers contain multiple threads, which is tunable, to perform the actual execution. 
%The insight behind this design is that such separation enables different optimization opportunities that are \emph{only} applicable to certain layer, and a proper collaboration between two optimized layer would deliver an overall efficient solution. 
%We will introduce our designs in detail in the following discussion.

\subsection{Streaming Processing Layer}
\label{subsec:sp}
%Our key design principle of SP-Layer is to minimize the synchronization overhead caused by transaction processing.
The design of SP-Layer is based on the conventional pipelined and parallel processing model adopted in many modern DSPSs~\cite{profile}. 
Specifically, each operator may be carried by one or more physical threads which communicate with each other through message passing.
Transaction slices generated during stream computation are asynchronously propagated to TP-Layer for state accesses, as shown in Figure~\ref{fig:SP-Layer}(a). 
To achieve this, the SP-Layer groups required information 
% Specifically, transaction requests are packed with necessary information 
(e.g., keys of to-be-accessed records, operations, values to be updated, event timestamps) 
into one package and sends it to TP-Layer. 
If SP-Layer expects any return values from the TP-Layer due to the existence of read operations, 
then it marks the corresponding input event as \emph{incomplete}, and creates a \emph{place-holder} pointing to the to-be-accessed records in the internal state, which will be filled up the TP-Layer in the future. 

%When an operator's processing of an event involves a \emph{write} operation to the internal state, corresponding information (i.e., destination record, value to be updated, triggering timestamp) are packed and delivered to the TP-Layer.
%When the stream processing involves \emph{read} operation, \system mark the process of the events as \emph{incomplete} and creates a \emph{place-holder}, which points to the targeting records in the internal state. The place-holder will be filled up \emph{later} by the TP-Layer with correct version of targeting records.

SP-Layer periodically broadcasts to the input stream \emph{watermarks}, which act as synchronization signals in the system, as shown in Figure~\ref{fig:SP-Layer}(b). 
A watermark guarantees that no subsequent input events has a timestamp larger than any events a prior of it.
Once an operator receives the watermark, it needs to ensure all its received events have been fully processed in both layers, and the corresponding watermark has been forwarded to downstream operators. 
Only when all operators in the same stage receive a watermark, TP-Layer starts to process all of its received transaction slices. 
In doing so, TP-Layer is involved {sequentially} for different stages of the application and therefore guarantees that transaction slices of the same transaction is executed strictly \emph{sequentially}.
%Therefore, transaction slices of the same input event are processed sequentially. 

The interval size of two subsequent watermark can be tuned by user, which plays an important role in tuning system throughput and processing latency.
Having a large interval, the system needs to buffer more events, and process more events once watermark received. This potentially increase system processing latency. 
Conversely, having a small interval size, the system throughput may drops, which may end up with more input events queued up, and eventually leads to high processing latency. 
We evaluate the effect of watermark interval in our experiments.

%Under a given watermark interval, the system may not be able to finish its computation before the inserting of the next watermark.
%\system resolves this issue by automatically regulating its input stream speed. 
%Specifically, it automatically tune its input stream rate to its sustainable value, which is measured as the current system throughput between two subsequent watermarks.
\begin{figure}
\centering
    \includegraphics*[width=0.4\textwidth]{SP-Layer.pdf}
    \caption{An example execution trace in \system.}                   
    \label{fig:SP-Layer}
\end{figure}

\subsection{Transaction Processing Layer}
%\textbf{Transaction Processing Layer.}
\label{subsec:txn}
%TP-layer processes transaction requests arrived between two consecutive watermarks from the SP-Layer.
%The use of watermark in SP-Layer turns \emph{infinite} number of streaming transaction processing into \emph{finite} (i.e., finite number of transactions between two subsequent watermarks). 
%TP-Layer essentially processes a batch of transactions arrived between two consecutive watermarks from the SP-Layer. 
%In the following discussion, 
We now discuss how TP-Layer parallelize the execution of transaction slices while still guaranteeing ordering-preserving consistency.
%As SP-Layer may be stalled waiting for place-holders to be filled, 

As discussed before, TP-Layer can only start processing when watermark is received by all operators in the same stage.
%needs to process received transaction slices as fast as possible while maintaining event ordering. 
Fortunately, it has the complete overview of all transaction slices to be handled without worrying future transaction slices. This is because they are guaranteed to be triggered by future events happening later than the current watermark. 
This allows the TP-Layer to aggressively extract higher concurrency from a batch of transaction slices arrived between every two consecutive watermarks.
%due to the use of watermark allows \system to process transaction requests in batches. 

The ordering consistency requirement essentially determines the sequences of conflicting operations of each data items (e.g., a write operation must precede a read operation on the same item if write operation has a timestamp smaller than the counter part). 
Based on this observation, our key idea is that
%to partition all received transaction slices into groups of operations on different records (called \emph{operation chains}). 
operations on each record from all transaction slices can be decomposed and maintained in sorted (in the event order) data structures, called \emph{operation chains}.
As a consequence, transaction slices are not immediately processed as inspired by prior work~\cite{Faleiro:2014:LET:2588555.2610529}. 
Instead, they are decomposed and all resulting operations are buffered in operation chains, which will be eventually fully processed \emph{as late as} subsequent watermark arrived. 

%\textbf{Evaluation pushdown.}
%One straightforward approach of supporting $RW$ operation is to fetch record from the shared store (done by TP-Layer), return to the SP-Layer, who optionally applies modification to the value, and then issues a write operation with the updated value to the shared store (done by TP-Layer again).

%In order to speed up this process, 
%There are two benefits of such design. 
%First, the input event triggering RW operation does not have to stalled (similar to the case of write-only), which is especially critical for latency-sensitive applications.
%Second, it also creates the opportunity of applying more advanced optimization techniques such as

The main disadvantage of this approach is that earlier submitted transaction slices can not be immediately processed until subsequent watermark arrived to trigger the processing. This potentially increases processing latency. 
However, such lazy evaluation approach brings many opportunities in extracting higher concurrency while preserving event order. As a result, the significant improved performance minimizes the latency penalty.
Another limitation is that, the write-set of a transaction must be deducible before the transaction begins. 
This can be solved either through explicit write-set declaration by the program that submits the transaction or through analysis of the transaction at runtime~\cite{Ren2014}. 

%The difference is that operator may send data access requests to shared state. 
We illustrate the mechanism with the same previous example in Figure~\ref{fig:SP-Layer}.
%previous example as depicted in Figure~\ref{fig:SP-Layer}.
%illustrate the execution of SP-Layer with the previous example in Figure~\ref{fig:SP-Layer}. 
$R_1'$ and $R_1''$ concurrently process $e_1$ and $e_3$, and generate read requests to records x, y, z and x, z as transaction slices, as $txn_1$ and $txn_3$, respectively. 
At the same time, $W_1'$ processes $e_2$ and generates write requests to records x and y as transaction slice as $txn_2$. 
Assuming the subsequent watermark has a timestamp of 4, and there are only these three transaction slices in the system. 
As $e_2$ is in between $e_1$ and $e_3$, $txn_1$ shall read unmodified state of x, y, while $txn_3$ shall read updated state of $x$.
$txn_3$ shall read the updated value of $x$ as a result of executing $txn_2$, while $txn_1$ shall read the original value of $x$ and $y$. 
As illustrated in Figure~\ref{fig:TP-Layer}, there are three operation chains constructed in this example, r$_1$(x)$\rightarrow$w$_2$(x)$\rightarrow$r$_3$(x), r$_1$(y)$\rightarrow$w$_2$(y) and r$_1$(z)$\rightarrow$r$_3$(z).
Those operation chains can run independently in parallel, while still preserving event ordering without locks or any other synchronization primitives. 

\tony{
To reduce round-trip communication overhead between two layers, we \emph{push evaluation down} to the TP-Layer as much as possible. 
%Transaction slices involving write-only operation are simply propagated to TP-Layer and does not require return to the SP-Layer.
Specifically, transaction slices involving \emph{write-only} operation are simply propagated to TP-Layer and does not require return to the SP-Layer.
%is packed with the corresponding information (i.e., targeting record, value to be updated, triggering timestamp) and delivered to TP-Layer.
%Transaction slices involving \emph{read-only} operation are similarly packed and delivered to TP-Layer. In addition, SP-Layer marks the corresponding input event as \emph{in-complete}, and creates a \emph{place-holder} which points to the targeting records in the internal state. 
%The place-holder will be filled up \emph{later} by TP-Layer with correct version of targeting records.
%In both cases, the TP-Layer will guarantee the complete processing of the received read/write requests.
For \emph{read-write} operation, the involved read and modification operations can be also propagated as function calls, which will be executed in TP-Layer without return to the SP-Layer. 
%the input event triggering RW operation does not have to stalled (similar to the case of write-only), which is especially critical for latency-sensitive applications.
However, in some cases, the operation's write request of one record depends on the value of different records in the internal state. This resulting in \emph{cross-records operation dependency}.
% of how we can handle such issue by leveraging multi-versioning.
We can leverage on \emph{multi-versioning} to handle such issue. Specifically, the system keeps multiple versions of each involved record. All cross-record read requests are replaced by function calls to query the desired version of depended record. When the depended record is not ready, the function call has to be stalled. In the extreme case, the system falls back to sequential execution when it needs to wait until operation chain of the dependent records is fully processed. Figure~\ref{fig:TP_transform2} illustrates the mechanism. 
A write operation with timestamp of 2 on $y$ depends on the value of x at the same timestamp. It requires to apply a modification function on the value of $x$ to obtain a temporal value $t$, which is then used to update $y$. This is a typical read-modify-write operation and could resulting in cross-records operation dependency in \system.
Both the read and modify operations are replaced by a function call with $x$ at timestamp $2$ as input. This replacement removes cross-records operation dependency since read or write operations on $x$ will not be blocked by operations on $y$. However, if there is no update to $x$ at all and its timestamp is fixed at a smaller value than 2, then the function call has to be stalled until operation chains of $x$ are fully processed to guarantee correctness. 
%The system falls back to sequential execution under such extreme case.
}

As each operation chain can be processed independently, 
\emph{work-stealing} are applied to achieve better load balancing. 
Specifically, operation chains formed a task pool, where system threads can concurrent fetch task (as one operation chain) from the pool to work with. 
Once a thread finishes its task, it fetches another until there is no task left in the pool. 
Some threads may be still working on long operation chains, and those idle threads (i.e., failed to obtain new task from the pool) will cooperatively join the process. An atomic counter of each operation chain is used to prevent the same operation being processed more than once.
%threads of TP-Layer, once finished its assigned workload, can cooperatively help to work on the long operation chain processed by other threads.

Figure~\ref{fig:operation_chain} illustrates how the operation chains are actually constructed.
Each operation chain is implemented by a concurrent skip-list, ordered by the triggering event timestamp.
The operation chains are hash indexed by destination record ($d\_record$)'s primary key.
In this way, operation chains can be constructed concurrently with minimum synchronization overhead.
%without relying on any synchronization primitives.
%multiple execution threads in the SP-Layer can concurrently construct the operation chains.
The \emph{core-map} partitions workload among cores, this helps the system to concurrently submit operation chains as tasks.
%and once a watermark received, operation chains can be processed concurrently. 
%As each operation chain can be processed independently, work-stealing are applied to achieve better load balancing. 



\begin{figure}
\centering
\begin{minipage}[b]{0.5\textwidth}
  \begin{minipage}[b]{0.58\textwidth}
    \includegraphics*[width=\textwidth]{TP-Layer.pdf}
    \caption{Operation-chain based processing model.}                   
    \label{fig:TP-Layer}
  \end{minipage}   
  \begin{minipage}[b]{0.41\textwidth}
    \includegraphics*[width=\textwidth]{TP_transform2.pdf}
    \caption{Resolving cross-records dependency.}                   
    \label{fig:TP_transform2}
  \end{minipage}  
\end{minipage}    
\end{figure}


\begin{figure*}
\centering
%  \begin{minipage}[b]{0.28\textwidth}
%    \includegraphics*[width=\textwidth]{TP_transform2.pdf}
%    \caption{Resolving cross-records dependency.}                   
%    \label{fig:TP_transform2}
%  \end{minipage}  
  \begin{minipage}[b]{0.74\textwidth}
    \includegraphics[width=\textwidth]{figure/operation_chain.pdf}
    \caption{Operation chains construction.}
    \label{fig:operation_chain}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
      \includegraphics[width=\textwidth]{figure/microbenchmark.pdf}   
      \caption{DAG of our modified YCSB benchmark.}                   
      \label{fig:microbenchmark}
  \end{minipage} 
\end{figure*}


%The first hash partitioned map to partition the submission work among multiple threads to speed up the task submission process. 
%Specifically, 
%records are split into $m$ partitions according to their primary key. 
%The 
%When operators are ready to submit to perform the actual operation, each operator will be responsible for submitting tasks from a subset of partitions. $m$ shall be at least equal or larger to the number of operators in a stage.

%\begin{figure*}
%\centering
%    \includegraphics*[width=0.9\textwidth]{operation_chain.pdf}
%    \caption{Operation chains construction.}                   
%    \label{fig:operation_chain}
%\end{figure*}





%\begin{figure}
%\centering
%    \includegraphics*[width=0.35\textwidth]{TP_transform2.pdf}
%    \caption{Resolving cross-records dependency.}                   
%    \label{fig:TP_transform2}
%\end{figure}



%\textbf{NUMA-awareness.}
%We reserve one socket to preparing data sources including shared store as well as data generator operators. 
%The remaining 7 CPU sockets are used to launch other operators. 
%To maximize NUMA locality, \system explicitly 
%Following previous work~\cite{Porobic:2012:OHI:2350229.2350260}, we configure the data placement of the shared store in an island-aware manner. 
%We divided shared state into disjoint subset on each CPU socket with approximate size.
%Specifically, TP-Layer is divided into $n$ processing instances, each instance is responsible for a range of transactions. 
%In all experiments, the total size of shared state is kept constant and is also range-partitioned across all instances, and we use the same total number TP-Threads for processing.

%\textbf{System durability.}
%Fault tolerance in distributed systems has been extensively studied~\cite{}. 
%Inspired by recent work from Fernandez et al.~\cite{seep}, \system achieves fault tolerance by exposing internal operator states (i.e., creating operator snapshot). Figure~\ref{fig:snapshot} illustrates the high level idea of such asynchronize snapshot mechanism.
%Specifically, each operator periodically store its state into reliable sources (e.g., disks). 
%The storing can be triggered by watermark, which is the same that we use for transaction processing.
%As the operator snapshot is created asynchronously, there is no stall of the overall processing resulting in better performance.
%

%\begin{figure}
%\centering
%    \includegraphics*[width=0.5\textwidth]{faulttolerance.pdf}   
%    \caption{Asynchronize snapshot fault tolerance.}                   
%    \label{fig:snapshot}
%\end{figure} 


%Based on further dependency analysis, \system performs transformations on the operation chain to improve execution efficiency.
%There are mainly two transformation operations as depicted in Figure~\ref{fig:TP_transform}.
%\textbf{Shared-reading optimization.}
%First, results of consecutive read operations to the same data item can be shared as there is no update (write). This effectively reduces the total amount of work. 
%This can be done by sequentially consolidate read operations from the top until it encounters a write operation. Whenever it encounters a write operation, it starts a new consolidation process. 
%The reading results need to write back to the place-holders maintained by SP-Layer. 
%After the consolidation, there is going to be only one thread to write back the reading value of each data item to the place-holders of SP-Layer. 
%Note that, this does not necessary require it to write multiple times in case multiple operators of SP-Layer are waiting for it. Instead, it only needs to write once, and the value is going to be shared reading by SP-Layer.

%\textbf{Processing optimizations.}
%Our chain-based processing model guarantees the execution correctness but its concurrency is limited by the number of data items involved.
%Furthermore, it unnecessarily forces a serial execution of r$_1$(z) and r$_3$(z), as there is no update of item $z$ anyway.
%
%Based on further dependency analysis, \system extract more concurrency from the naive operation chains.
%First, read operations to the same record shall be parallelized. 
%This can be done by parallelizing read operations of each operation chain from the top until it encounters a write operation. 
%Whenever it does, it starts a new parallelizing process. 
%Second,  write operation can be also parallelized by maintaining multiple versions of each data item. 
%This creates the potential of increasing the level of concurrency.
%Essentially, we can break each operation chain into multiple sub-chains with the break point of each write operation. 
%The read after write dependency needs still be maintained, and are hence regrouped together.
%
%In order to better utilizes the multicore resources, \system parallelizing the execution on a chain if the length of the chain is longer than a certain threshold. 
%After the chain optimization, \system submit all operation chains (incl. sub-chains) to $n$ concurrently running threads and applies work-stealing scheme to achieve workload balancing.
%%In order to better utilizes the multicore resources, TP-Layer needs to decide the correct level of concurrency during transformation. 
%%Consider there are $n$ TP-threads, it is ideal to transform the operation chains into $n$ groups with \emph{balanced} workloads. 
%%We can first greedily parallelizing all write operations, and then organize the resulting sub-graphs into $n$ balanced groups.
%%This organizing problem can be naturally mapped into the multi-way partition problem~\cite{Korf:2009:MNP:1661445.1661531}. 
%%Specifically, given a set of numbers, the goal is to partition it into $k$ subsets so that the subset sums are as similar as possible.
%%The problem is NP-hard, and there are several efficient heuristic algorithms for it~\cite{Korf:1998:CAA:297463.297468}.
%
%\begin{figure}
%\centering
%    \includegraphics*[width=0.45\textwidth]{TP_transform.pdf}
%    \caption{Example of transformation optimizations.}                   
%    \label{fig:TP_transform}
%\end{figure}

%
%Specifically, the total size of shared state is kept constant and is range-partitioned across all partitions, and we use the same total number TP-Threads for processing.
%We vary the number of partitions of the shared store and the distribution of TP-Threads across partitions.
%
%We denote the configuration as \emph{n}ISL, where $n$ represents the number of partitions.
%For example, we may configure 1ISL

%
%We ensure that each database instance is optimally deployed. That is, each database process is bound to the cores
%within a single socket (minimizing NUMA effects) when pos-
%sible, and its memory is allocated in the nearest memory
%bank.
%

