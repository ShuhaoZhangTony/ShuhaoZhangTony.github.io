\section{Challenges}
\label{sec:motivation}
In this section, we use a real-world health care application to motivate our work.
Then, we summarize the challenges of supporting stateful stream processing on modern multicore machines, followed by an overview of existing designs. 

\subsection{Motivating Example}
\label{sec:examples}
%As discussed in Section~\ref{sec:background}, 
Supporting scalable stateful stream processing is challenging. 
This is especially true when out-of-core state exist.
% <<<<<<< HEAD
% However, streaming applications containing shared states are prevalent.
% Figure~\ref{fig:application} (a) shows a real-world heath-care application~\cite{acep} where { \em A} and { \em C} are two operators continuously monitoring behaviors of health-care workers (HCWs). Both operators' run-time action are determined by the status of each HCW,  which is stored in a shared status table. At the same time, active rules, e.g., operator {\em B} in this example, are updating HCW's status upon detecting certain behavior (e.g., update to warning status if the worker didn't perform sanitize properly). 
% =======
However, streaming applications containing out-of-core states are prevalent and have been adopted in a wide range of application domains from healthcare~\cite{acep}, sensor monitoring~\cite{botan2012transactional} and leaderboard maintenance~\cite{S-Store}.
Due to space limitation, we focus on the hospital infection control application~\cite{acep} to illustrate the usage of out-of-core state in stream processing. 

Figure~\ref{fig:application} shows the hospital infection control 
example where the input event streams contain the
real-time behaviors, such as ``exit'', ``sanitize'', ``enter'', 
of the health care workers (HCWs). 
These events are generated continuously by the sensor devices deployed on HCWs and are analyzed by several continuously running operators to detect if there exists any violation of hospital hygiene rules in HCWs.
Status of all HCWs are stored in a single status table.

Operator $A$ continuously detects whether any worker under ``safe'' status exits the hospital and enters later without performing sanitation.
 % if the worker is under ``safe'' status (a read to the status table). 
The detected results from $A$ are sent to its downstream operator $B$, which updates the corresponding HCW's status into ``warning''. 
Subsequently, the following operator $C$ detects whether a worker wears mask before exiting the hospital if he is under ``warning'' status.

The status of all HCWs are stored in a single status table, 
which is read or written by different operators in the streaming DAG.
While parallelizing the computation triggered by a single stream 
event $e_1$ can be easily achieved by 
partitioning the streams based on worker ID, a DSPS can hardly 
parallelize $e_1$'s computation with that caused by 
its following event $e_2$.
This is because $e_2$'s computation is unaware of the access pattern of
that triggered by $e_1$, and the DSPS has to use certain concurrency
control protocol to guarantee the state consistency,
if concurrent reads and writes to the state is enabled.
This problem is exacerbated if more complex analytical queries
such as scan and range lookup exist.
% it is crucial to control such concurrent updates and accesses so as to assure the correct execution logic. 
In the following subsections, we will further elaborate why DSPSs can confront severe performance overhead due to the concurrent accesses to those out-of-core states.

%and { \em C} are two operators continuously monitoring behaviors of health care workers (HCWs). Both operators' run-time action are determined by the status of each HCW,  which is stored in a shared status table. 
%At the same time, operator {\em B} is updating HCW's status upon detecting certain behavior (e.g., update to warning status if the worker didn't perform sanitize properly). 
% >>>>>>> 62f6f905bf36aa317e953f1e62279c5e39ab28e6
% which requires supporting shared state in stream processing due to the introduce of \textit{active rules} in complex real-time analytics. 
% An example workload described in ACEP is shown in Figure~\ref{fig:application} (a).
%It is therefore curtical to guarantee the correct interaction
%and they may read and update the same shared rule table. \yingjun{more explanation}
%Another sensor monitoring application that utilizes shared state~\cite{botan2012transactional} is shown in Figure~\ref{fig:application} (b). 
%{ \em A} and { \em B} are two concurrently running operators, where { \em B} updates the machine specifications according to user requests while { \em A} monitors machine sensor input and generate status report with reference to machine specifications.
%Recently, MeeHan et al.~\cite{S-Store} attempted to fuse OLTP and streaming processing together and developed the SStore system. 
%The third representative application~\cite{S-Store} is shown in Figure~\ref{fig:application} (c).
%Similar to the previous two applications, different operators may update and read {multiple} shared states {concurrently}.
%In all aforementioned applications, 
\begin{figure}
\centering
    \includegraphics*[width=0.45\textwidth]{application_acep.pdf}
    \caption{Hospital infection control application.}                   
    \label{fig:application}
\end{figure}

\subsection{System Design Challenges}
\label{sec:challenge}
We now summarize the challenges of out-of-core state management in two different aspects.

\emph{1) Ordering-preserving consistency.}
In order to maintain the consistency of out-of-core state and to guarantee the correctness of stream computation results,
a DSPS that attempts to concurrently manipulate the out-of-core state must perform access operations with ACID guarantees.
 % in the face of concurrent accessing to the shared store, each operation is executed with ACID guarantees. 
Due to its similarity to transaction processing, we henceforth use \emph{transaction} to denote 
the set of state access operations triggered by a single incoming event.
% each such state access operation.
% Guaranteeing ACID properties~\cite{Bernstein:1996:PTP:261193} has long been the correctness criterion for transaction processing.
% To guarantee ACID properties during transaction processing,
Classic transaction processing theories indicate that a stateful 
DSPS can implement existing 
% a system can adopt 
% Many 
\emph{concurrency control} (CC) protocols to serialize incoming state access operations.
% have been proposed to improve the transaction processing performance while preserving ACID. 
However, in the context of stream processing, a stateful DSPS 
must not only serialize the state accesses in an order
that is conflict-equivalent to any certain serial schedule,
but also enforce that the resulting order is aligned with 
that implied by the timestamps attached to the sequence of streaming events~\cite{S-Store,botan2012transactional}. 
% that supports streaming application 
% requires the state accesses are not only performed in a 
% not only ACID but also needs to make sure conflicting read or write operations are serialized in the event order, which is more restrictive than existing CC. 
% At its minimum, stateful stream processing needs to guarantee the processing of state management operations follows event arrival ordering
For example, a read operation triggered by an event shall never see an updated state that is modified by a write operation triggered by ``future'' events. This is critical as streaming events are chronologically ordered.
%\yingjun{don't understand. rewrite.} An additional desired property is to guarantee real event occurrence ordering, and this requires the system to be able to handle out-of-arrival events. In this paper, we assume input stream are totally ordered for simplicity.


%However, in this example, $B$ requests the update too late and $C$ will reads an earlier status, which contradicts with the input event sequence.
%ordered together (detailed in Section~\ref{sec:arch})
%TSP system needs to ensure that $txn_i$ and $txn_j$ are executed \emph{as if} they are serially executed in the order of $txn_i \rightarrow txn_j$.
\begin{figure}
\centering
    \includegraphics*[width=0.45\textwidth]{example_trace.pdf}
    \caption{Example execution trace leads to incorrect computation results under conventional concurrency control protocols.}                   
    \label{fig:example_tace}
\end{figure}

We use basic timestamp-ordering concurrency control (T/O CC)~\cite{Bernstein:1981:CCD:356842.356846} as an example to explain why conventional concurrency control protocols, which are widely used in OLTP database systems, fail to guarantee DSPS's internal state consistency.
Figure~\ref{fig:example_tace} illustrates an execution trace that leads to incorrectness.
Upon receiving input event, each operator performs some computing tasks, and request access (read or write) to the out-of-core states. 
We denote input event as $e_1$ and $e_2$, where the numbers represent their timestamps. 
Suppose operator $A$ detects a violation for worker $w1$, and the detection triggers operator $B$ to update $w1$'s status. In the meanwhile, operator $C$ requests to read status of the same worker.
It is required that $C$ reads the correct status of $w1$ after $B$'s update as $e_1$ happens ahead of $e_2$. 

%According to T/O CC algorithm, for any read or write operation, the system rejects the request if the transaction's timestamp is less than the timestamp of the last write to that tuple. 
Let us first denote $txn_1$ and $txn_2$ as the state management requests triggered by $e_1$ and $e_2$, respectively. 
Without loss of generality, let us assign timestamp of both transaction to be the same of their triggering event.
For simplicity, let us assume there are only these two transactions in the system. 
Figure~\ref{fig:example_tace} shows that $txn_2$ arrives at the system earlier, it is accepted as there is {no} write operation ahead of it updating the timestamp of the record. $txn_1$ arrives later, which is also accepted, as $txn_2$, being a read operation, does not update timestamp.
As a result, both $txn$s will successfully commit but their serial order would be $txn_2 \rightarrow txn_1$, which violates the event order (i.e., $txn_2$ will incorrectly read the original status of $w1$). 
%Let us first denote $A$'s read operation and $B$'s write operation as two transactions, $txn_1$ and $txn_2$, respectively. 
%Further, let us denote $ts$ as their corresponding transaction timestamp.
%If we assign $txn_2.ts < txn_1.ts$, then both $txn$s will successfully commit but their serial order would be $txn_2 \rightarrow txn_1$,
%which violates the event order (i.e., $txn_2$ will incorrectly read the original value of $x$). 
%On the other hand, if we assign $txn_2.ts > txn_1.ts$, then $txn_2$ will successfully commit as it arrives to the system earlier according to T/O CC algorithm. 
%Conversely, $txn_1$ will be aborted as its writes come too late, this is equally unacceptable in shared state management.
Similarly, other conventional CC either results in incorrect serial order or has to abort one transaction that eventually results in incorrect serial order after restart.
In other words, conventional CC protocols are not yet ready for such \emph{event-driven} transaction execution.
%More formally, denote $e_i$, $e_j$ as two input event, and $e_i$ has a timestamp $e_i.ts$ smaller than $e_j$ (i.e., $e_i$ happens earlier).
%Assume the processing on them will involve transactions $txn_i$ and $txn_j$ respectively, and system needs to ensure that $txn_i$ and $txn_j$ are executed \emph{as if} they are serially executed in the order of $txn_i \rightarrow txn_j$.
%Consequently, they can be fit into a common computational model that integrates the stream processing capabilities and transactional management capability.
%We implement four state-of-the-art competing approaches in \system. 

\emph{2) High parallelism demand.}
Different from conventional OLTP databases, stateful stream processing to support \emph{hybrid} data analytics and state management at the same time.  
That is, the application may contain complex analytics tasks, and the state management operations are optionally embedded in the analytics.
For better performance, DSPSs must extract parallelisms aggressively.
One commonly adopted approach is to have each operator run in multiple physical threads with its internal state partitioned, as we discussed in Section~\ref{sec:background}.
However, the goal of parallelizing the computations triggered by different events has fundamental conflicts with the \emph{ordering-preserving consistency} requirement. 
It hence results in a non-trivial question of how to scale stateful stream processing on modern multicore processors.

\subsection{Existing Solutions}
\label{subsec:CC}
%Witnessing the emergence of modern commodity machines with massively parallel processors, researchers and
%practitioners nowadays can perform ultra-fast stream processing on a single multicore machine.
%However, fully exploiting the computation power delivered by multicore machines can be challenging.
Recently, several specialized algorithms have been proposed support real-time analytics and concurrent accesses to internal state in stream processing~\cite{Affetti:2017:FIS:3093742.3093929,S-Store,botan2012transactional,acep}. 
% These algorithms target at analyzing large volumes of streaming data on the fly while consistently updating the shared stores in a concurrent manner. 
However, these algorithms can confront severe scaling bottlenecks, due to the requirement of tracking timestamp ordering of the event-triggered transactions.
%To validate how different designs perform on modern multicore machine, we implement five alternative algorithms including one without guaranteeing timestamp alignment, three prior proposed solutions and \algo. 
%All five algorithms are implemented in a main memory DSP system

As shown in Figure~\ref{fig:overview}(a), from a high level point of view, existing solutions commonly adopt an integrated approach where stream computation and state management are handled in a single tier.  For example, the triggered stream computation and subsequent state management requests of one event are processed by the same execution thread. 
%In summary, existing solutions can be represented as the following two approaches: 
% 1) the system maintains a monotonically-increasing counter for every event of the input stream, and uses the counter to guarantee the updating order~\cite{acep}.
% However, this introduces many unnecessary synchronizations, and turns read into write, for example, even a read-only transaction needs to first update the global counter.
% 2) The system pessimistically relies on a two-phase-locking based implementation~\cite{botan2012transactional} but is known to scale poorly on multicores~\cite{}.
% 3) The system relies on workload partitioning to avoid the need of concurrency control~\cite{S-Store}; however, this method is not always feasible.
%Despite its simplicity, 
Such a design results in a single contention point which limits the system performance on modern multicore processors. 
%maximizes the reuse of existing systems and optimizations in decades, but 
%can severely limit scalability on multicore processors, mainly due to the global synchronization barrier that enforces lock ordering among triggering %events. 
%which can be either global control variable~\cite{acep} or a centralized lock manager~\cite{botan2012transactional}. 
%This results in a single contention point and limit the system performance on modern multi-core processors. 

%First, they ensure that all input events are totally ordered, which comes at a cost. 
%In particular, a global synchronization barrier is usually required to enforce ordering,
%which can be either global control variable~\cite{acep} or a centralized lock manager~\cite{botan2012transactional}.
%Both approaches have a single contention point and limit the system performance on modern multi-core processors.
%Second, an event may not trigger transactions to the shared store (e.g., an event may be simply filtered), 
%so synchronizing the execution for all input events is unnecessary.

\begin{figure}
\centering
    \includegraphics*[width=0.45\textwidth]{overview_task.pdf}
    \caption{A comparison between conventional monolithic and \system design.}                   
    \label{fig:overview}
\end{figure}

We now describe three existing concurrency control schemes for internal state management. We listed the comparisons to \system in Table~\ref{tbl:cc}.

\textbf{Lock-Ahead 2PL (LAL).}
Wang et al.~\cite{acep} described a strict two-phase locking (S2PL) based implementation that allows multiple streaming transactions run concurrently while maintaining ordering-preserving consistency. 
The algorithm is very similar to the classic S2PL, but depends on a ``lock-ahead'' mechanism to make sure that locks are granted in the correct event order. 
Specifically, it requires to execute a lock-adding operation as soon as an input event arrives but before processing starts. 
The locks (either read or write locks) are henceforth invoked strictly in their triggering event order.
 % and guarantees ordering-preserving consistency.
%Subsequently, ordering-preserving consistency are guaranteed. 
However, the lock-ahead mechanism has to synchronize the execution for every single input event, potentially causing severe reduction in system concurrency.

\textbf{Low-Water-Mark (LWM).}
Observing the problem in LAL, Wang et al.~\cite{acep} further improve the algorithm by adding multi-versioning support. We call the improved algorithm \emph{low-water-mark (LWM)}. LWM leverages a global synchronization primitive to guard the transaction processing sequence: write operations must be performed monotonically in the event order, but read operations are allowed to execute as long as it is able to read the correct version of data (i.e., as long as its timestamp is smaller than LWM). 
Botan et al~\cite{botan2012transactional} also proposed a similar mechanism that use a centralized transaction manager (TM) to synchronize transactions. It exposes read and write APIs to streaming operators. When a read/write operation is called, TM has to check whether the corresponding transactions are allowed to proceed. The transaction
execution order is also guaranteed by the centralized TM.
% Although the detail implementation is not described, we suspect that TM is still rely on some forms of synchronization primitives to determine the event sequence, similar to Wang's approach~\cite{acep}. 

\textbf{Partition-based Scheduler (PAT).}
S-Store~\cite{S-Store} fuses OLTP and streaming into one engine. 
It is built based on top of H-Store, which is a shared-nothing deterministic database system~\cite{Kallman:2008:HHD:1454159.1454211}. 
We call the mechanism \emph{PAT}.
% S-Store performs quite well if the internal states are partitionable.
S-Store splits the streaming application's internal states into multiple disjoint \emph{partitions}. The computation on each sub-partition is performed by a single thread. To guarantee the state consistency,
S-Store requires using partition-level locks to synchronize the accesses.
While performing quite well on partitionable states, S-Store's PAT mechanism cannot perform well on out-of-core states, as acquiring
partition-level locks on cross-partition states can cause significant performance overhead.
%When transaction workload can be perfectly partitioned, conflicts are completely avoided so there is no need to worry the violation of event sequence targetting at records from different partitions.
%However, when transaction needs to access multiple partitions, each affected partition needs to be carefully locked to guard event sequence, which may severely underutilized the abundant parallelism of multicore processors.
%, and in fact, 
% The original implementation of S-Store only use a single core for data access~\cite{S-Store}, which severely underutilized the abundant parallelism of multicore processors. We implement a partition-based algorithm in \system, called PAT, following the similar idea of S-Store. Instead of having one global synchronization primitive as in LAL and LWM, a group of synchronization primitives are used under PAT scheme, where each partition owns one of them for ensure transactions on it are strictly aligned with event order. 
%This increases concurrency
%This increases efficiency as threads compete for multiple (instead of one) locks for enforcing ordering. However, we show later that the performance of such approach is highly depended on the ratio of multi-partition transactions. Its performance drops rapidly with a small portion of multi-partition transactions.

\begin{table}
\centering
  \caption{Comparisons in concurrency control schemes for internal state management}
  \label{tbl:cc}
  \includegraphics[width=0.45\textwidth]{figure/CC.pdf}
\end{table}
